{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/13020363/UTS_ML2019_ID13020363/blob/master/assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hayr7XXy21fb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Assignment 1\n",
        "https://colab.research.google.com/drive/1eWPD-bW845fU2xufHzgB29SQvgxF05a7#scrollTo=hayr7XXy21fb\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<br>\n",
        "<dir align='middle'>\n",
        "  <font size='5'>\n",
        "An Evaluation of the article\n",
        "  </font>\n",
        "</dir>\n",
        "<br><br><br>\n",
        "Submitted by<br>\n",
        "Yudi Chen<br>\n",
        "13020363<br>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "<p>\n",
        "  <font size=\"20\" color=\"blue\" face='Times New Roman' >\n",
        "    Introduction\n",
        "  </font><br>\n",
        "  <font size='4' face='Times New Roman'>\n",
        "The paper that I choose is called 'A Tutorial on Support Vector Machines for Pattern Recognition'. That article states the basic ideas and the principle of Support Vector Machines, as well as recent applications and extensions. In this paper, I will summarize the contents of the chosen paper and give some viewpoints after some evaluations.\n",
        "  </font><br>\n",
        "  <font size=\"20\" color=\"blue\" face='Times New Roman'> \n",
        "    Content\n",
        "  </font><br>\n",
        "  <font size='4' face='Times New Roman'>\n",
        "Before introducing the principle of Support Vector Machine, the article claims the background and groundwork, in which what is VC dimension and structural risk minimization. Firstly, VC dimension is a property of a set of functions {f(α)}. Suppose there are l observations. Each one includes a vector xi ∈ Rn, i = 1, . . . , l and the associated yi. If labeling a set of l points in all possible 2l ways, one member of the {f(α)} can assign those labels. This is a process that the set of functions shatter the set of points and the VC dimension is the maximum number of training points which can be shattered by the {f(α)}. Another concept is structural risk minimization (SRM). According to Christopher J.C. Burges (1998), the value of empirical risk and actual risk is influenced by the special function, while VC is influenced by the class of functions. The best option is finding the subset of the chosen function whose subset has minimized the risk bound. To find the subset, the way is that the class of functions is divided into some subsets. Then train some machines for each subset to minimize the empirical risk. Finally, select the trained machine who has the minimum sum of empirical risk and VC confidence.<br>\n",
        "\n",
        "Next section is linear Support Vector Machines (SVMs) and nonlinear SVMs .\n",
        "\n",
        "![](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/1805e19e9fa6a4c140c531bc0dca8016ee75257b/9-Figure5-1.png)<br>\n",
        "Figure 1: Separable Linear SVMs<br>\n",
        "    \n",
        "The optimal separable linear result with a two-dimensional case is like the Figure 1. The points which are highlighted by circles are support vectors, because the solution will change if they remove. And then explaining nonlinear cases, the paper introduces Lagrangian formulation and Karush-Kuhn-Tucker (KKT) conditions, apart with the description of when is global solutions or uniqueness. After the tutorial of SVMs, it states the optimization problem and the related solution. The solution includes two phases, the first phase is finding the nearest face along the search direction which is along the gradient and then computing analytically the optimal point along the search direction if the maximum along search direction locates among current point and the nearest face. In next section, the article describes the performance of SVMs, apart from limitation and extension.\n",
        "  </font><br>\n",
        "  <font size=\"20\" color=\"blue\" face='Times New Roman'>\n",
        "    Innovation\n",
        "  </font><br>\n",
        "  <font size='4' face='Times New Roman'>\n",
        "On one hand, the main points are the concepts of linear SVMs and nonlinear SVMs. Some papers focus on the way of using and only tell what is SVMs without the origin or the deduction process, whereas, the article introduces not only the basic concepts but also the deduction process of formula. Although the concepts and the way of using the concepts are not new, the extra part explanation helps readers understand more and use the technology with skills. For instance, according to Sunil Ray (2017), the SVMs can deal with the two kinds of points which distribute like two concentric circles (Figure 2), but he does not explain the reason. The reason is clear after reading this article. \n",
        "\n",
        " ![](https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_8.png)<br>\n",
        "Figure 2: distribution like two circles<br>\n",
        "\n",
        "On the other hand, the way of resolving problems is interesting and new. For example, when it describes solving the equality constrained problem, the article says that either the Newton method or Projection methods has the disadvantage that some constrains become inactive although they are active at a time. However, the new method computes using little extra cost. Besides, the article figures out the way of optimizing the performance of the SVMs by using SVM-like classifier (Gap Tolerant Classifiers). It proofs that Structural Risk Minimization can improve the performance of Gap Tolerant Classifiers and then uses this way on SVMs. From my perspective, this is a good method, because it is not controlled well for a new skill. If doing some tests on the similar classifiers, the method can be found easier and the result can be believable. Then adjust the method and use it on the origin.\n",
        "  </font><br>\n",
        "  <font size=\"20\" color=\"blue\" face='Times New Roman'>\n",
        "    Technical quality\n",
        "  </font><br>\n",
        "  <font size='4' face='Times New Roman'>\n",
        "This article’s technical quality is reasonable. The final purpose is introducing that the structural risk minimization can improve the performance of SVMs. However, there main concepts and principles need to state before do that. Therefor the structure of this article is from the basic groundwork (VC dimension and structural risk minimization), what is Support Vector Machine, improving the performance of SVMs to the limitation. From my perspective, this framework improves the technical quality, because these sections have relationships between each other, the readers cannot go ahead if one of them is unreasonable. In details, every section explains concisely and the logic is clear. The best method is using structural risk minimization with SVM-like classifier, because the SVM-like classifier has the similar features with the SVM and the result is similar. Besides, the previous classifier is more popular to people, so it can be controlled better as well as the readers can make senses easier. In the introduction of linear SVMs and nonlinear SVMs, the article classifies the types of SVMs into separable linear SVMs, non-separable linear SVMs and nonlinear SVMs respectively. Because the situations of these types are different, they are used in different scenarios and the same method is not suitable to all of them, the technology must be discussed in different situations. In summary, the technical quality is good.\n",
        "  </font><br>\n",
        "  <font size=\"20\" color=\"blue\" face='Times New Roman'>\n",
        "    Application and X-factor\n",
        "  </font><br>\n",
        "  <font size='4' face='Times New Roman'>\n",
        "In my opinion, the applications are appropriate for the corresponding types of SVMs, because the given examples are based on the special types of SVMs. Due to the data-handling capability, the speed of SVMs can be limited when handling a large amount of data, both in training data and testing data, therefore, I recommend that improving the data-handling capability is a good direction so that the technology can be used in more domains. Another recommendation is that developing the skills based on the special types of SVMs. Because the SVMs have many types and they have different feathers, figuring out a method which is appropriate for all the types seems hard to achieve in short term. In my opinion, concentrating on the most widely used technology is the direction of further developments.\n",
        "  </font><br>\n",
        "  <font size=\"20\" color=\"blue\" face='Times New Roman'>\n",
        "    Presentation\n",
        "  </font><br>\n",
        "  <font size='4' face='Times New Roman'>\n",
        "In conclusion, this article describes the methods of improving the performance of SVMs. Although the two methods are simple, they are effective, and they can make more contributions if continuing to tune the methods. On the other hand, the article uses many terms and formulas. From my perspective, it shows the article professional but it has barriers to prevent new men from reading deeply, because they need to spend more time on searching materials to learn the terms and formulas. Fortunately, this article explains the methods and concepts clearly. It could be more helpful to make senses, if showing or explaining more basic methods.\n",
        "   </font><br>\n",
        "  <font size=\"20\" color=\"blue\" face='Times New Roman'>\n",
        "    References\n",
        "  </font><br>\n",
        "  <font size='4' face='Times New Roman'>\n",
        "[1] Burges, C.J., 1998. A tutorial on support vector machines for pattern recognition. Data mining and knowledge discovery, 2(2), pp.121-167.\n",
        "\n",
        "[2] Sunil R. 2017, Understanding Support Vector Machine algorithm from examples\n",
        ", viewed 19 August 2019, < https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/>.\n",
        "  </font>\n",
        "</p>\n"
      ]
    }
  ]
}